---
title: "Our R project"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages
Packages we'll look at today:

- odbc, readxl, readr, dbplyr for data access
- tidyverse for data manipulation
- DataExplorer for providing automated EDA of our data
- modelr, rsamples for sampling
- recipes for performing feature engineering 
- glmnet, h2o, FFTrees for building models
- yardstick, broom for evaluation
- rmarkdown for documentation

## Working with databases
We need a database connection before we can do anything with our database.
```{r}
library(DBI)
library(odbc)

driver = "SQL Server" 
server = "fbmcsads.database.windows.net"
database = "WideWorldImporters-Standard"
uid = "adatumadmin"
pwd = "Pa55w.rdPa55w.rd"

con<-dbConnect(odbc(),
               driver = driver, 
               server = server,
               database = database,
               uid = uid,
               pwd = pwd)
```


Now that we have a DB connection, we can write SQL in a code chunk.
```{sql connection=con}
select top 5 * from flights
```

We can use dbplyr to construct dplyr commands that work on the DB.
```{r}
library(tidyverse)
library(dbplyr)
flights_tbl<-tbl(con, "flights")

flights_tbl %>% 
  filter(month<=6) %>% 
  group_by(origin) %>% 
  summarise(n = n(), 
            mean_dist= mean(distance)) %>% 
  show_query()
```

We can also work with tables that aren't in the default schema.
```{r}
purchaseorders_tbl<-tbl(con, in_schema("purchasing","purchaseorders"))

purchaseorders_tbl %>% 
  top_n(5)
```

We can use the `Id()` function from DBI to work with schema more generically within a database. This means we aren't restricted to just SELECT statements.

```{r error=TRUE}
# Create a schema to work in - errors if already exists
dbGetQuery(con,"CREATE SCHEMA DBIexample")
# Write some data - drop & recreate the table if it exists already
dbWriteTable(con, "iris", iris, overwrite=TRUE) 
# Read from newly written table
head(dbReadTable(con, "iris"))
# Read from a table in a schema
head(dbReadTable(con, Id(schema="20774A",table="CustomerTransactions")))
# If a write method is supported by the driver, this will work
dbWriteTable(con, Id(schema="DBIexample", table="iris"), iris, overwrite=TRUE)
```

Some of our code could fail in that section so we used `error=TRUE` to be able to carry on even if some of the code errored. Great for optional code or things with bad connections.

## Exploratory Data Analysis

```{r eval=FALSE}
flights_tbl %>% 
  as_data_frame() %>% 
  DataExplorer::GenerateReport()
```

Questions arising from the basic report:

1. Why is there a day with double the number of flights?
3. Why is there negative correlation between `flight` (flight number) and `distance`?
4. Do we need to anything about missings or can we just remove the rows?

Things to implement later in the workflow due to the EDA:

1. We need to address the high correlation between time columns
2. We need to group low frequency airline carriers
3. Bivariate analysis

### Answering our questions

> Why is there a day with double the number of flights?

Are there duplicate rows?

```{r}
flights_tbl %>% 
  filter(day==15) %>% 
  distinct() %>% 
  summarise(n()) %>% 
  as_data_frame() ->
  distinct_count

flights_tbl %>% 
  filter(day==15) %>% 
  summarise(n())%>% 
  as_data_frame() ->
  row_count

identical(row_count, distinct_count)

```


But are the number of rows unusual?
```{r}
library(ggplot2)
flights_tbl %>% 
  group_by(day) %>% 
  summarise(n=n(), n_unique=n_distinct(flight)) %>% 
  as_data_frame() %>% 
  ggplot(aes(x=day, y=n)) +
    geom_col()
```
Looks like the jump in the histogram is an artifact of binning the data. d'oh!


### Bivariate analysis
```{r}
flights_tbl %>% 
  select_if(is.numeric) %>% 
  as_data_frame() %>% 
  gather(col, val, -dep_delay) %>% 
  filter(col!="arr_delay",
         dep_delay<500) %>% 
  ggplot(aes(x=val, y=dep_delay)) +
    geom_bin2d() +
    facet_wrap(~col, scales = "free")+
    scale_fill_gradientn(colours = viridisLite::viridis(256, option = "D"))
```


## Sampling

### Theory / Info
Our options for sampling data with a large class imbalance are:

- Downsampling takes as many majority rows and there are minority rows
    + No overfit from individual rows
    + Can drastically reduce training data size
- Upsampling or over sampling repeats minority rows until they meet some defined class ratio
    + Risks overfitting
    + Doesn't reduce training data set
- Synthesising data makes extra records that are like the minority class
    + Doesn't reduce training set
    + Avoids some of the overfit risk of upsampling
    + Can weaken predictions if minority data is very similar to majority
    
We need to think about whether we need to k-fold cross-validation explicitly.

- Run the same model and assess robustness of coefficients
- We have an algorithm that needs explicit cross validation because it doesn't do it internally
- When we're going to run lots of models with hyper-parameter tuning so the results are more consistent

We use bootstrapping when we want to fit a single model and ensure the results are robust. This will often do many more iterations than k-fold cross validation, making it better in cases where there's relatively small amounts of data.

Packages we can use for sampling include:

- modelr which facilitates basic, bootstrap, and k-fold crossvalidation strategies
- rsample allows us to bootstrap and perform a wide variety of crossvalidation tasks
- recipes allows us to upsample and downsample
- synthpop allows us to build synthesised samples

### Practical
First we need to split our data into test and train.
```{r}
flights_tbl %>% 
  as_data_frame() ->
  flights

flights %>% 
  mutate(was_delayed= ifelse(arr_delay>5,"Delayed", "Not Delayed"),
         week = ifelse(day %/% 7 > 3, 3, day %/% 7 )) ->
  flights

flights %>%   
  modelr::resample_partition(c(train=0.7,test=0.3)) ->
  splits

splits %>% 
  pluck("train") %>% 
  as_data_frame()->
  train_raw

splits %>% 
  pluck("test") %>% 
  as_data_frame()->
  test_raw
```

During the investigation, we'll look at the impact of upsampling. We'll see it in action in a bit. First prepping our basic features!

```{r}
library(recipes)

basic_fe <- recipe(train_raw, was_delayed ~ .)

basic_fe %>% 
  step_rm(ends_with("time"), ends_with("delay"),tailnum, flight,
          minute, time_hour, day) %>% 
  step_naomit(all_predictors()) %>% 
  step_naomit(all_outcomes()) %>% 
  step_zv(all_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_other(all_nominal(),threshold = 0.03)  ->
  colscleaned_fe

colscleaned_fe <- prep(colscleaned_fe, verbose = TRUE)
colscleaned_fe

train_prep1<-bake(colscleaned_fe, train_raw)
```

Now we need to process our numeric variables.

```{r}
colscleaned_fe  %>% 
  step_log(distance) %>% 
  step_num2factor(month, week, hour) %>% 
  step_rm(tailnum, dest) -> #hack!
  numscleaned_fe

numscleaned_fe <- prep(numscleaned_fe, verbose = TRUE)
numscleaned_fe

train_prep1<-bake(numscleaned_fe, train_raw)
```

W00t it's upsampling time!

```{r}
numscleaned_fe %>% 
  step_upsample(all_outcomes(), ratio=1) %>% 
  prep(retain=TRUE) %>% 
  juice() %>% 
  # hack because juice isn't reducing the column set
  bake(numscleaned_fe, .) ->
  train_prep2
```



## Building models
Decide which types of models you want to consider -- perhaps using Microsoft's lovely [cheat sheet](https://docs.microsoft.com/en-gb/azure/machine-learning/studio/algorithm-cheat-sheet). Then determine if any need any special processing to the data beyond what you've done so far.


### A basic logistic regression

We can use generalised linear model functionality to construct a logistic regression.

```{r}
glm_unbal<- glm(was_delayed ~ . -1 , "binomial", data = train_prep1)
glm_bal  <- glm(was_delayed ~ . -1 , "binomial", data = train_prep2)
```

Then we can see how these models are constructed and how they perform.

```{r}
glm_unbal
```

Fit measures on our *training* data
```{r}
library(broom)
glance(glm_unbal)
```

Get the coefficients
```{r}
tidy(glm_unbal)
```

Get the fitted data
```{r}
head(augment(glm_unbal))
```

Plot predicted's vs actuals

```{r}
glm_unbal %>% 
  augment() %>% 
  ggplot(aes(x=.fitted, group=was_delayed, fill=was_delayed)) +
  geom_density(alpha=.5) +
  geom_vline(aes(xintercept=0))
```

#### Prep and predict on test data
```{r}
test_raw %>% 
  bake(numscleaned_fe, .) %>% 
  modelr::add_predictions(glm_unbal,var = "glm_unbal")  ->
  test_scored
```


```{r}
test_scored %>% 
  ggplot(aes(x=glm_unbal, group=was_delayed, fill=was_delayed)) +
  geom_density(alpha=.5) +
  geom_vline(aes(xintercept=0))
```

But how many did we get right etc?

```{r}
library(yardstick)
test_scored %>% 
  mutate(glm_unbal_class=as.factor(
      ifelse(glm_unbal<0, "Delayed", "Not Delayed"))) %>% 
  conf_mat(was_delayed, glm_unbal_class)
```


```{r}
test_scored %>% 
  mutate(glm_unbal_class=as.factor(
      ifelse(glm_unbal<0, "Delayed", "Not Delayed"))) %>% 
  accuracy(was_delayed, glm_unbal_class)
```

### A glmnet for feature selection
Use regularization to smooth results by modifying coefficients of variables. 
```{r}
library(glmnetUtils)

glmnet_unbal<- glmnet(was_delayed~., 
                      train_prep1, 
                      family="binomial",
                      alpha=0.5, 
                      intercept=FALSE)
glmnet_unbal
```

```{r}
glance(glmnet_unbal)
```

How many variables in the model at different levels of regularization?
```{r}
plot(glmnet_unbal, label = TRUE)
```

What level of variance if explained at each step?
```{r}
set.seed(1050104)
glmnet_unbal_cv<-cv.glmnet(was_delayed~., 
                      train_prep1, 
                      family="binomial",
                      alpha=0.5)
```

How do different weights perform?

```{r}
plot(glmnet_unbal_cv)
```


```{r}
coef(glmnet_unbal_cv, s = "lambda.min")
```
#### Prep and score test data
```{r}
test_scored$glmnet_unbal_cv<-as.vector(predict(glmnet_unbal_cv,
                                      test_scored,
                                      na.action = na.pass ))

```

```{r}
test_scored %>% 
  ggplot(aes(x=glmnet_unbal_cv, group=was_delayed, fill=was_delayed)) +
  geom_density(alpha=.5) +
  geom_vline(aes(xintercept=0))
```
